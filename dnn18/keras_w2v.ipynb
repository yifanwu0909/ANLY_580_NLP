{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yifan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dot, Embedding, Activation, Input, Reshape\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Word2Vec\n",
    "\n",
    "This notebook is a simplified version of word2vec. The model creates a semantic space by training a model that predicts which word appeared in the context of another word. The model won't achieve 100% accuracy - think of all the words that can appear within five words of the context \"dog\" and you'll see how it's an impossible objective. However, since words with similar meanings/grammar tend to have similar contexts, the embedding layer is a very useful semantic space.\n",
    "\n",
    "This is a self-supervised task because it doesn't require separate annotation. The \"annotation\" for a word is the words that appear around it in a large corpus.\n",
    "\n",
    "## Corpus to input\n",
    "\n",
    "Real word2vec rose to prominence in large part because of how efficient it is - how it processed as much language data within hours as previous word-embedding models processed within weeks. Training data is \"free\" - think all of Wikipedia - because the task is self-supervised.\n",
    "\n",
    "I'm using Moby Dick to show how the data is made and what the model looks like -- but that's much too small for a real, usable semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2371 words in vocabulary.\n",
      "Most common:[',', 'the', '.', 'of', 'and', 'a', 'to', 'in', ';', 'that']\n"
     ]
    }
   ],
   "source": [
    "txt_fn = \"mobydick.txt\"\n",
    "\n",
    "# these models require a closed vocabulary\n",
    "# get word frequency from the whole corpus\n",
    "vocab_count = {}\n",
    "with open(txt_fn, 'r') as novel:\n",
    "    for line in novel:\n",
    "        tokens = word_tokenize(line.strip().lower())\n",
    "        #tokens = line.strip().lower().split() #laziest possible preprocessing\n",
    "        for t in tokens:\n",
    "            if t not in vocab_count:\n",
    "                vocab_count[t] = 0\n",
    "            vocab_count[t] += 1\n",
    "            \n",
    "# set a threshold for frequency\n",
    "min_count = 5\n",
    "vocab_count = {k:v for k,v in vocab_count.items() if v>=min_count}\n",
    "\n",
    "# typically, vocabulary is sorted with most frequent words first\n",
    "vocab_list = [k for k,v in sorted(vocab_count.items(), key=lambda x:x[1], reverse=True)]\n",
    "\n",
    "# dictionaries map strings to their indices in the list\n",
    "# these will be the rows for word embeddings in our matrix\n",
    "lookup = {word:i for i,word in enumerate(vocab_list)}\n",
    "\n",
    "vocab_size = len(vocab_list)\n",
    "print(\"{} words in vocabulary.\\nMost common:{}\".format(vocab_size, vocab_list[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram (one word) corpus probabilities\n",
    "\n",
    "The most frequent words of language are extremely frequent. We don't need to train the embedding for \"the\" 234x as much as the embedding for \"fish.\" Here, we make sampling probabilities using some code from the gensim implementation of word2vec. The frequency ranks are preserved (\"the\" is still more important than \"fish\"), but the curve is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word frequency\n",
    "vocab_frequencies = [vocab_count[word] for word in vocab_list]\n",
    "total_tokens = sum(vocab_frequencies)\n",
    "\n",
    "vocab_proportion = np.asarray(vocab_frequencies)/float(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampling when more frequent than 112.194\n",
      "[0.12029566 0.14235709 0.19685154 0.21047309 0.22202446 0.25428078\n",
      " 0.2629592  0.28225839 0.29667513 0.34049365] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "scaled_prob = np.zeros((len(vocab_frequencies)))\n",
    "\n",
    "threshold_count = 1e-3*total_tokens\n",
    "print(\"downsampling when more frequent than {}\".format(threshold_count))\n",
    "\n",
    "for i,v in enumerate(vocab_frequencies):\n",
    "    word_probability = (np.sqrt(v / threshold_count) + 1) * (threshold_count / v)\n",
    "    scaled_prob[i] = min(word_probability, 1)\n",
    "    \n",
    "print(scaled_prob[:10],scaled_prob[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2ae05729ef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEaCAYAAAAG87ApAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ae036e0b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the flattened curve\n",
    "\n",
    "downscaled_freqs = vocab_frequencies * scaled_prob\n",
    "downscaled_percents = downscaled_freqs / downscaled_freqs.sum()\n",
    "\n",
    "plt.plot(downscaled_percents, label=\"scaled\")\n",
    "plt.plot(vocab_proportion, label=\"raw\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Type Rank\")\n",
    "plt.ylabel(\"Token Probability\")\n",
    "plt.title(\"Scaling Unigram Probability for Negative Sampling\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus to get context windows\n",
    "\n",
    "Objective is to match words with their context - this is \"self-supervised\" and no data is held out for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator to read corpus one line at a time - don't put the whole thing in memory\n",
    "# Yield center word and window of context\n",
    "# THIS is where sampling happens for word2vec, effectively increasing window size\n",
    "def corpus_stepper(corp_file, context_size, probabilities, lookup):\n",
    "    context_needed = (context_size*2)+1\n",
    "    tokens = []\n",
    "    cf = open(corp_file, 'r')\n",
    "    while True: # loop through forever\n",
    "        if len(tokens) >=context_needed:\n",
    "            left_context = tokens[:context_size]\n",
    "            token = tokens[context_size]\n",
    "            right_context = tokens[context_size+1:context_needed]\n",
    "            yield (token,left_context+right_context)\n",
    "            tokens = tokens[1:]\n",
    "            #print(tokens)\n",
    "        else: # get another line, possibly another file\n",
    "            new_tokens = cf.readline()\n",
    "            if not new_tokens: #end of file\n",
    "                cf.close()\n",
    "                cf = open(corp_file, 'r')\n",
    "                new_tokens = cf.readline()\n",
    "            line_tokens = word_tokenize(new_tokens.strip().lower())\n",
    "            line_inds = [lookup[w] for w in line_tokens if w in lookup]\n",
    "            #print(line_inds,line_tokens)\n",
    "            # downsample using precomputed probabilities\n",
    "            rand_keep = np.random.random() # 0 - 1\n",
    "            line_inds = [ind for ind in line_inds if probabilities[ind]>=rand_keep]\n",
    "            tokens.extend(line_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "['whale', '(', 'supplied', 'late', 'pale', 'coat', 'heart', 'body']\n",
      "pale\n",
      "['(', 'supplied', 'late', ')', 'coat', 'heart', 'body', 'brain']\n",
      "coat\n",
      "['supplied', 'late', ')', 'pale', 'heart', 'body', 'brain', 'see']\n",
      "heart\n",
      "['late', ')', 'pale', 'coat', 'body', 'brain', 'see', 'now']\n",
      "body\n",
      "[')', 'pale', 'coat', 'heart', 'brain', 'see', 'now', 'ever']\n",
      "brain\n",
      "['pale', 'coat', 'heart', 'body', 'see', 'now', 'ever', 'old']\n",
      "see\n",
      "['coat', 'heart', 'body', 'brain', 'now', 'ever', 'old', 'queer']\n",
      "now\n",
      "['heart', 'body', 'brain', 'see', 'ever', 'old', 'queer', 'handkerchief']\n",
      "ever\n",
      "['body', 'brain', 'see', 'now', 'old', 'queer', 'handkerchief', 'gay']\n",
      "old\n",
      "['brain', 'see', 'now', 'ever', 'queer', 'handkerchief', 'gay', 'known']\n"
     ]
    }
   ],
   "source": [
    "# instantiate our corpus stepper\n",
    "window = 4\n",
    "context_gen = corpus_stepper(txt_fn, context_size=window, probabilities=scaled_prob, lookup=lookup)\n",
    "for i in range(200): # get past some weirdness at beginning\n",
    "    next(context_gen)\n",
    "for i in range(10):\n",
    "    w,cxt = next(context_gen)\n",
    "    print(vocab_list[w])\n",
    "    print([vocab_list[c] for c in cxt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "This model tries to assign the real context word the highest probability of the whole vocabulary. However, the vocabulary size is prohibitively large - here's it's a few thousand, but models trained on internet data have millions of vocabulary items.\n",
    "\n",
    "Instead, we use \"negative sampling.\" The real vocabulary item is the positive sample. The randomly chosen vocabulary items are negative samples. (They are random but have a sampling probability.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 618  195   54  487 1460 1051   21  111   86   70]\n",
      "['wonder', 'whaling', 'like', 'com-', 'proceed', 'exactly', 'all', 'men', 'only', 'though']\n"
     ]
    }
   ],
   "source": [
    "#negative samples are just based on unigram frequency\n",
    "vocab_size = scaled_prob.shape[0]\n",
    "negative_samples = np.random.choice(vocab_size, size=(10,), p=downscaled_percents)\n",
    "print(negative_samples)\n",
    "print([vocab_list[i] for i in negative_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train using a generator so we can generate new negative samples every epoch (pass through the training data)\n",
    "# a larger corpus might also not fit into memory\n",
    "\n",
    "def train_generator(corpus_gen, unigram_probabilities, num_neg):\n",
    "    \n",
    "    while True:\n",
    "        word,contexts = next(corpus_gen)\n",
    "\n",
    "        X = np.zeros((len(contexts), 1), dtype=np.int32)\n",
    "        X += word # all of X is the same center word\n",
    "        y = np.zeros((len(contexts), num_neg+1), dtype=np.int32)\n",
    "        i = 0\n",
    "        for context in contexts:\n",
    "            y[i,0] = context # index 0 is the positive\n",
    "            y[i, 1:] = np.random.choice(vocab_size,\n",
    "                                size=num_neg,p=unigram_probabilities)\n",
    "            i += 1\n",
    "        if i == 0: # no contexts, don't feed empty array\n",
    "            continue\n",
    "        # truncate in case we lost some out-of-vocab contexts\n",
    "        X = X[:i] \n",
    "        y = y[:i]\n",
    "\n",
    "        target = np.zeros((i,1), dtype=np.int32) # this says \"index 0 of y is always correct\"\n",
    "        yield ([X,y], target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the generator\n",
    "tg = train_generator(context_gen, downscaled_percents, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of generator:\n",
      "[array([[460],\n",
      "       [460],\n",
      "       [460],\n",
      "       [460],\n",
      "       [460],\n",
      "       [460],\n",
      "       [460],\n",
      "       [460]], dtype=int32), array([[ 123,   92,  167, 1019,  379,   42, 1879,   14, 1270,   19,    3],\n",
      "       [  48,  112,    1,  479, 2367, 2053,   10,   11, 1144, 1545, 1049],\n",
      "       [ 147,  319,   31,   34,   39, 2156,  338,    2,   46,    9,   85],\n",
      "       [  75,   13,  254,  443,   12,  239, 1784, 1616,   99,  226,   18],\n",
      "       [1976,  647,    3,    1, 2224,   46,   13,  906,  123,   98,  695],\n",
      "       [1017,  148, 2274,  285,   67,   51,   44,    4,  638, 2093,   40],\n",
      "       [ 260,  171,  235,  275,   23,   74,  412,    6,   18,   60,    2],\n",
      "       [1126,    1, 2287,   38,    7, 1347,  167,  369,   39,   82,    0]],\n",
      "      dtype=int32)]\n",
      "\n",
      "center word: queer\n",
      "samples:\n",
      "postitive: see\n",
      "\tnegative: ['queequeg', 'should', 'dutch', 'indian', 'were', 'phantoms', 'he', 'roll', 'for', 'of']\n",
      "postitive: now\n",
      "\tnegative: ['could', 'the', 'live', 'perched', 'knee', 'his', \"'\", 'colour', 'spoke', 'lungs']\n",
      "postitive: ever\n",
      "\tnegative: ['get', 'on', 'there', 'me', 'calculating', 'thy', '.', 'which', 'that', 'any']\n",
      "postitive: old\n",
      "\tnegative: ['it', 'face', 'keep', 'i', 'called', 'news', 'lurked', \"n't\", 'full', 'as']\n",
      "postitive: handkerchief\n",
      "\tnegative: ['plain', 'of', 'the', 'position', 'which', 'it', 'nose', 'see', 'did', 'aboard']\n",
      "postitive: gay\n",
      "\tnegative: [':', 'advancing', 'thee', 'into', 'upon', 'what', 'and', 'nigh', 'arrive', 'have']\n",
      "postitive: known\n",
      "\tnegative: ['know', 'true', 'stood', '!', 'such', 'cape', 'to', 'as', 'man', '.']\n",
      "postitive: nations\n",
      "\tnegative: ['the', 'incidentally', 'had', 'in', 'outer', 'should', 'looking', 'me', 'down', ',']\n"
     ]
    }
   ],
   "source": [
    "inputs,target = next(tg)\n",
    "print(\"output of generator:\")\n",
    "print(inputs)\n",
    "print(\"\\ncenter word: {}\".format(vocab_list[inputs[0][0,0]]))\n",
    "print(\"samples:\")\n",
    "for word_list in inputs[1]:\n",
    "    print(\"postitive: {}\\n\\tnegative: {}\".format(vocab_list[word_list[0]],[vocab_list[i] for i in word_list[1:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set some hyperparameters\n",
    "vocab_size=len(lookup)\n",
    "embed_size=64\n",
    "num_neg=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context_input (InputLayer)      (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_vec (Embedding)            (None, 1, 64)        151744      word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "context_vec (Embedding)         (None, 11, 64)       151744      context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 11)        0           word_vec[0][0]                   \n",
      "                                                                 context_vec[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 11)        0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 11)           0           activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 303,488\n",
      "Trainable params: 303,488\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_in = Input((1,), dtype=\"int32\", name=\"word_input\")\n",
    "# embedding for the input word\n",
    "word_embed = Embedding(vocab_size, embed_size, name=\"word_vec\")\n",
    "word = word_embed(word_in)\n",
    "\n",
    "context_in = Input((num_neg+1,), dtype=\"int32\", name=\"context_input\")\n",
    "# embedding for all the context words\n",
    "context_embed = Embedding(vocab_size, embed_size, name=\"context_vec\")\n",
    "context = context_embed(context_in)\n",
    "\n",
    "# a score of how well the context and word match\n",
    "dotp = Dot(axes=[2,2], name=\"dot\", normalize=True)([word,context])\n",
    "\n",
    "# scale the scores so they sum to 1. \"probability\" that each is the right word\n",
    "prediction = Activation(\"softmax\")(dotp)\n",
    "prediction = Reshape((num_neg+1,))(prediction)\n",
    "model = Model(inputs=[word_in,context_in], outputs=[prediction,])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical crossentropy means we're pitting all the of the outputs against each other\n",
    "# sparse means that our target is the index of the right answer, not the full dense matrix of probabilities\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.3708\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 2.3147\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 12s 12ms/step - loss: 2.3603\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.3699\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.3874\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.3844\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.3316\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.3293\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.3136\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 2.3432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3621bf28>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every time we pass 1000 batches (ie words and contexts), call that an epoch\n",
    "\n",
    "model.fit_generator(tg, steps_per_epoch=1000, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the word vectors out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word_input', 'context_input', 'word_vec', 'context_vec', 'dot', 'activation_1', 'reshape_1']\n"
     ]
    }
   ],
   "source": [
    "print([l.name for l in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(2371, 64)\n"
     ]
    }
   ],
   "source": [
    "# it's easy to extract the weights from your model\n",
    "embedding_layer = model.get_layer('word_vec')\n",
    "embedding_weights = embedding_layer.get_weights()\n",
    "#keras returns weights in lists, even when there's only 1 in the layer\n",
    "print(len(embedding_weights))\n",
    "embedding_weights = embedding_weights[0]\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or\n"
     ]
    }
   ],
   "source": [
    "# try cosine similarity - but the results might not be great with just a little training of a tiny corpus!\n",
    "\n",
    "target_word = \"whale\"\n",
    "target_ind = lookup[target_word]\n",
    "all_sims = cosine_similarity(embedding_weights, embedding_weights[target_ind].reshape((1,-1)))\n",
    "all_sims[target_ind] = 0 # otherwise similarity with self is max\n",
    "max_sim = np.argmax(all_sims)\n",
    "print(vocab_list[max_sim])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
