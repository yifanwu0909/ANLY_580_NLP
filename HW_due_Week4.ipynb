{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a).  Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$', '.', '/', '/', '.']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(f):\n",
    "    my_text = open(f).read()\n",
    "    return my_text\n",
    "\n",
    "text = load(\"corpus.txt\")\n",
    "\n",
    "pattern_a = r'''(?x)           # set flag to allow verbose regexps\n",
    "          [^a-zA-Z0-9\\s_]    # searches for non-alphanumeric characters.\n",
    "'''\n",
    "\n",
    "all_puncts = nltk.regexp_tokenize(text, pattern_a)\n",
    "all_puncts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b). Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jane Smith', '$13.04', '09/09/1994', 'Pike Place Fish Market']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "      \\$\\d+(?:\\.\\d+)? \n",
    "    | \\d{1,2}/\\d{1,2}/\\d{4}\n",
    "    | [A-Z][a-z]+(?=\\s[A-Z])(?:\\s[A-Z][a-z]+)+\n",
    "'''  \n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.43 With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), and NLTK's frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 'French_Francais-Latin1'), -0.11264822134387353), ((2, 'German_Deutsch-Latin1'), 0.006153846153846176), ((1, 'Spanish-Latin1'), 0.21652173913043482), ((3, 'English-Latin1'), 0.44341880341880346)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import *\n",
    "from nltk.corpus import udhr\n",
    "from nltk.corpus import genesis\n",
    "from nltk import spearman_correlation\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "languages = []\n",
    "\n",
    "def target_text_prep(text):\n",
    "\n",
    "    mystery_text = [list(word.lower()) for word in text if word.isalpha()]\n",
    "    mystery_text = [item for sublist in mystery_text for item in sublist]\n",
    "    fd_mystery_text = FreqDist(mystery_text)\n",
    "    mystery_ranks = list(ranks_from_sequence(fd_mystery_text))\n",
    "\n",
    "    return mystery_ranks\n",
    "\n",
    "def language_pool_prep(fids):\n",
    "\n",
    "    languages = [fileid for fileid in fids if re.findall('Latin1', fileid)] \n",
    "    udhr_corpus = [[list(word.lower()) for word in udhr.words(language) if word.isalpha()] for language in languages]\n",
    "    udhr_corpus = [[item for sublist in language for item in sublist] for language in udhr_corpus]\n",
    "    languages = list(enumerate(languages))\n",
    "    language_freq_dists = [FreqDist(language) for language in udhr_corpus]\n",
    "    language_ranks = [list(ranks_from_sequence(dist)) for dist in language_freq_dists]\n",
    "    return languages, language_ranks\n",
    "\n",
    "def spearman(mystery_ranks, language_ranks):\n",
    "\n",
    "    spearman_numbers = [] \n",
    "    for language in language_ranks:\n",
    "        number = spearman_correlation(language, mystery_ranks)\n",
    "        spearman_numbers.append(number)\n",
    "\n",
    "    return spearman_numbers\n",
    "\n",
    "\n",
    "\n",
    "fids = ['French_Francais-Latin1', 'Spanish-Latin1', 'German_Deutsch-Latin1', 'English-Latin1']\n",
    "text = gutenberg.words('austen-emma.txt')\n",
    "\n",
    "languages, language_ranks = language_pool_prep(fids)\n",
    "mystery_ranks = target_text_prep(text)\n",
    "spearman_numbers = spearman(mystery_ranks, language_ranks)\n",
    "zipped = list(zip(languages, spearman_numbers))\n",
    "answer = sorted(zipped, key=lambda x: x[1])\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
